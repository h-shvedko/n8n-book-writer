# WPI Content Factory PoC ‚Äî Setup-Anleitung

## Schnellstart (5 Minuten)

### 1. n8n installieren (falls noch nicht vorhanden)

**Option A: Docker (empfohlen)**
```bash
docker run -it --rm \
  --name n8n \
  -p 5678:5678 \
  -v n8n_data:/home/node/.n8n \
  n8nio/n8n
```

**Option B: npm**
```bash
npm install -g n8n
n8n start
```

√ñffne: http://localhost:5678

### 2. Workflow importieren

1. In n8n: **Workflows** ‚Üí **Import from File**
2. W√§hle: `wpi-content-factory-workflow.json`
3. Klicke **Import**

### 3. Credentials einrichten

**OpenAI API:**
1. Gehe zu **Settings** ‚Üí **Credentials**
2. Klicke **Add Credential** ‚Üí **OpenAI API**
3. F√ºge deinen API Key ein
4. Speichern

### 4. Erster Test-Run

1. √ñffne den importierten Workflow
2. Klicke auf den **üì• Book Request Form** Node
3. Klicke **Test Step** ‚Üí **Production URL**
4. F√ºlle das Formular aus:
   - **Book Slot ID:** `test-html-basics`
   - **Product Definition:** `Einf√ºhrung in HTML f√ºr absolute Anf√§nger. Behandelt Grundstruktur, wichtigste Tags, Formulare und semantisches HTML.`
   - **Target Audience:** `Absolute Beginners`
   - **Focus Areas:** `Semantic HTML, Accessibility`
   - **Number of Chapters:** `3`
5. Submitte das Formular
6. Beobachte die Execution in n8n

---

## Workflow-Konfiguration im Detail

### Agenten-Prompts anpassen

Jeder Agent hat einen **System Prompt**, der sein Verhalten definiert. Du findest ihn im jeweiligen AI Node unter **Messages** ‚Üí **System**.

**Beispiel: Writer Agent anpassen**
1. √ñffne den Node **‚úçÔ∏è Writer Agent**
2. Bearbeite den System Prompt
3. F√ºge WPI-spezifische Tone-of-Voice Regeln hinzu

### Human-in-the-Loop konfigurieren

Der Workflow pausiert beim **‚è∏Ô∏è Wait for Approval** Node und wartet auf menschliche Freigabe.

**Optionen:**
- **Email**: Konfiguriere den **üìß Send for Approval** Node mit echten SMTP-Daten
- **Slack**: Ersetze den Email-Node durch einen Slack-Node
- **Webhook**: Nutze die Webhook-URL direkt f√ºr externe Tools

### Output-Pfade anpassen

Die Nodes **üíæ Save Markdown** und **üíæ Save Exam Questions** speichern lokal. F√ºr Production:

**Google Drive:**
1. Ersetze die Nodes durch **Google Drive** Nodes
2. Konfiguriere OAuth Credentials

**GitHub:**
1. Ersetze durch **GitHub** Node
2. Automatisches Commit in ein Repository

---

## Erweiterte Konfiguration

### Anderes LLM verwenden

**Anthropic Claude:**
1. Ersetze **OpenAI** Nodes durch **Anthropic** Nodes
2. Passe die Message-Struktur an (Claude nutzt `human` statt `user`)

**Google Gemini:**
1. Nutze **HTTP Request** Node
2. Konfiguriere Vertex AI Endpoint
3. Oder nutze den Community Node `@n8n/n8n-nodes-google-ai`

### Web Search f√ºr Researcher

F√ºr echte Fakten-Recherche:

**Option 1: SerpAPI**
```
1. Erstelle SerpAPI Account (kostenlos bis 100 Suchen/Monat)
2. F√ºge HTTP Request Node vor Researcher ein
3. Query: Google Search via SerpAPI
4. Parse Ergebnisse und f√ºge sie zum Prompt hinzu
```

**Option 2: Perplexity API**
```
1. Perplexity API Key holen
2. HTTP Request an Perplexity API
3. Bereits aufbereitete Fakten-Antworten
```

### Code-Sandbox Integration

F√ºr echte Code-Validierung:

**Option 1: E2B (empfohlen)**
```javascript
// Im Coder Agent Node, f√ºge nach Code-Generierung hinzu:
const e2bResponse = await fetch('https://api.e2b.dev/v1/execute', {
  method: 'POST',
  headers: { 'Authorization': 'Bearer YOUR_E2B_KEY' },
  body: JSON.stringify({ code: generatedCode, language: 'javascript' })
});
```

**Option 2: n8n Code Node**
```javascript
// F√ºr einfache JavaScript-Validierung:
try {
  eval(generatedCode);
  return { valid: true };
} catch (e) {
  return { valid: false, error: e.message };
}
```

---

## Troubleshooting

### "Credentials not found"
- Stelle sicher, dass OpenAI Credentials den richtigen Namen haben
- Pr√ºfe die Credential-Referenz in den AI Nodes

### "JSON parse error"
- Der AI Output ist manchmal nicht perfektes JSON
- Die Code-Nodes haben Fallback-Logik eingebaut
- Passe bei Bedarf den System Prompt an ("Antworte NUR mit JSON")

### "Workflow h√§ngt bei Wait"
- Der Workflow wartet auf menschliche Freigabe
- Klicke den Resume-Link in der Email
- Oder: In n8n ‚Üí Executions ‚Üí W√§hle die Execution ‚Üí Resume

### "Rate Limit exceeded"
- OpenAI hat Limits (TPM/RPM)
- F√ºge Delays zwischen den AI-Calls hinzu
- Nutze `gpt-4o-mini` f√ºr g√ºnstigere/schnellere Calls

---

## Performance-Optimierung

### Parallele Verarbeitung

Aktuell werden Kapitel sequentiell verarbeitet. F√ºr Parallelisierung:

1. Ersetze **Split In Batches** durch **Split Out** (alle gleichzeitig)
2. Am Ende: **Aggregate** Node zum Zusammenf√ºhren
3. ‚ö†Ô∏è Achtung: Mehr API-Calls gleichzeitig = h√∂here Rate-Limit-Gefahr

### Caching

F√ºr wiederholte Runs mit √§hnlichem Input:

1. F√ºge **Redis** oder **File Cache** hinzu
2. Cache Research-Ergebnisse f√ºr 24h
3. Cache Blueprint f√ºr schnellere Re-Runs

---

## Kosten-Tracking

Um API-Kosten zu tracken:

1. F√ºge nach jedem AI-Node einen **Set** Node hinzu
2. Extrahiere `usage.total_tokens` aus der Response
3. Summiere am Ende und speichere in einer Tabelle

**Beispiel:**
```javascript
const usage = $json.usage;
const costPer1kTokens = 0.01; // GPT-4o
const cost = (usage.total_tokens / 1000) * costPer1kTokens;
return { tokens: usage.total_tokens, cost_usd: cost };
```

---

## N√§chste Schritte

1. **Teste den PoC** mit einem einfachen Thema
2. **Sammle Feedback** von Thorsten
3. **Iteriere** basierend auf WPI-spezifischen Anforderungen
4. **Integriere** mit WPI-Infrastruktur (LMS, Exam Platform)

---

**Fragen?** Kontaktiere: Hennadii Shvedko
